group: SMARTS
name: share/ppo

training:
  interface:
    type: independent
    population_size: -1
  config:
    # control the frequency of remote parameter update
    update_interval: 1
    saving_interval: 10
    batch_size: 256
    optimizer: Adam
    actor_lr: 0.01
    critic_lr: 0.05
    lr: 0.02
    tau: 0.01  # soft update
    grad_norm_clipping: 0.5

rollout:
  type: async
  stopper: simple_rollout
  stopper_config:
    max_step: 1000
  # metric_type: "simple"
  fragment_length: 1000
  num_episodes: 12
  episode_seg: 1
  terminate: "any"

evaluation:
  fragment_length: 25
  num_episodes: 100

env_description:
  creator: SMARTS
  config:
    #    scenario_name: "simple_push"
    env_id: marl
    scenario_configs:
      path:
        - scenarios/double_merge/cross
#        - scenarios/double_merge/nocross
#        - scenarios/intersections/4lane
#        - scenarios/intersections/4lane_sv
#        - scenarios/two_ways/bid
#        - scenarios/two_ways/bid_sv
      max_step: 1000
      agent_type: baseline-lane-control.yaml

algorithms:
  PPO:
    name: PPO
    model_config:
      actor:
        network: mlp
        layers:
          - units: 64
            activation: ReLU
          - units: 64
            activation: ReLU
        output:
          activation: False
      critic:
        network: mlp
        layers:
          - units: 64
            activation: ReLU
          - units: 64
            activation: ReLU
        output:
          activation: False

    # set hyper parameter
    custom_config:
      gamma: 0.99
      use_cuda: False  # enable cuda or not

global_evaluator:
  name: "generic"

dataset_config:
  episode_capacity: 1000000
  learning_start: 2560
